{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semana6_Sistemas_cognitivos_artificiales - CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdd5y9ju_ZfA"
      },
      "source": [
        "Importamos el dataset de piedra, papel y tijeras a la carpeta temporal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVcYcYUV3Bxn",
        "outputId": "54dfa8e2-af60-47ad-f742-b4b58a86cafe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip \\\n",
        "    -O /tmp/rps.zip\n",
        "  \n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip \\\n",
        "    -O /tmp/rps-test-set.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-15 16:04:32--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.217.128, 64.233.170.128, 108.177.11.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.217.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 200682221 (191M) [application/zip]\n",
            "Saving to: ‘/tmp/rps.zip’\n",
            "\n",
            "/tmp/rps.zip        100%[===================>] 191.38M   105MB/s    in 1.8s    \n",
            "\n",
            "2020-10-15 16:04:33 (105 MB/s) - ‘/tmp/rps.zip’ saved [200682221/200682221]\n",
            "\n",
            "--2020-10-15 16:04:34--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.204.128, 172.217.203.128, 142.250.97.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29516758 (28M) [application/zip]\n",
            "Saving to: ‘/tmp/rps-test-set.zip’\n",
            "\n",
            "/tmp/rps-test-set.z 100%[===================>]  28.15M  75.3MB/s    in 0.4s    \n",
            "\n",
            "2020-10-15 16:04:34 (75.3 MB/s) - ‘/tmp/rps-test-set.zip’ saved [29516758/29516758]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c3cXeQC_43J"
      },
      "source": [
        "Extraemos los datos y los descomprimimos:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoS4wle03D5a"
      },
      "source": [
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/rps.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = '/tmp/rps-test-set.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyka4XO-_70s"
      },
      "source": [
        "Creamos las carpetas para trabajar de forma organizada y las llenamos con los elementos correspondientes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y__lwwXQ3Ft_",
        "outputId": "b4072376-4957-4dbd-f8bd-f52caf15e845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "rock_dir = os.path.join('/tmp/rps/rock')\n",
        "paper_dir = os.path.join('/tmp/rps/paper')\n",
        "scissors_dir = os.path.join('/tmp/rps/scissors')\n",
        "\n",
        "print('total training rock images:', len(os.listdir(rock_dir)))\n",
        "print('total training paper images:', len(os.listdir(paper_dir)))\n",
        "print('total training scissors images:', len(os.listdir(scissors_dir)))\n",
        "\n",
        "rock_files = os.listdir(rock_dir)\n",
        "print(rock_files[:10])\n",
        "\n",
        "paper_files = os.listdir(paper_dir)\n",
        "print(paper_files[:10])\n",
        "\n",
        "scissors_files = os.listdir(scissors_dir)\n",
        "print(scissors_files[:10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training rock images: 840\n",
            "total training paper images: 840\n",
            "total training scissors images: 840\n",
            "['rock01-025.png', 'rock06ck02-016.png', 'rock02-030.png', 'rock02-097.png', 'rock06ck02-051.png', 'rock06ck02-080.png', 'rock07-k03-065.png', 'rock06ck02-106.png', 'rock02-023.png', 'rock04-075.png']\n",
            "['paper01-008.png', 'paper01-051.png', 'paper04-118.png', 'paper05-116.png', 'paper07-009.png', 'paper03-073.png', 'paper05-081.png', 'paper02-010.png', 'paper07-082.png', 'paper04-012.png']\n",
            "['scissors03-035.png', 'scissors03-048.png', 'scissors04-119.png', 'scissors04-007.png', 'scissors03-102.png', 'scissors04-050.png', 'testscissors03-091.png', 'scissors02-062.png', 'scissors01-002.png', 'testscissors02-098.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJV6LrYnAIPe"
      },
      "source": [
        "Aplicamos la tecnica de data augmentation para mejorar la eficiencia de la CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOateXOE3KBa",
        "outputId": "a64458f3-253b-4f69-9fa2-fdca26a51015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAINING_DIR = \"/tmp/rps/\"\n",
        "training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "\t    rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "VALIDATION_DIR = \"/tmp/rps-test-set/\"\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "\tTRAINING_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=126\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\tVALIDATION_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=126\n",
        ")\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, epochs=25, steps_per_epoch=20, validation_data = validation_generator, verbose = 1, validation_steps=3)\n",
        "\n",
        "model.save(\"rps.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2520 images belonging to 3 classes.\n",
            "Found 372 images belonging to 3 classes.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 148, 148, 64)      1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 74, 74, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 72, 72, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 3,473,475\n",
            "Trainable params: 3,473,475\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "20/20 [==============================] - 164s 8s/step - loss: 1.7924 - accuracy: 0.3619 - val_loss: 1.0868 - val_accuracy: 0.3226\n",
            "Epoch 2/25\n",
            "20/20 [==============================] - 164s 8s/step - loss: 1.0527 - accuracy: 0.4456 - val_loss: 1.0781 - val_accuracy: 0.3602\n",
            "Epoch 3/25\n",
            "20/20 [==============================] - 164s 8s/step - loss: 1.0024 - accuracy: 0.4917 - val_loss: 0.8531 - val_accuracy: 0.7043\n",
            "Epoch 4/25\n",
            "20/20 [==============================] - 163s 8s/step - loss: 0.9312 - accuracy: 0.5508 - val_loss: 0.5937 - val_accuracy: 0.6747\n",
            "Epoch 5/25\n",
            "20/20 [==============================] - 167s 8s/step - loss: 0.8552 - accuracy: 0.6369 - val_loss: 0.6201 - val_accuracy: 0.6667\n",
            "Epoch 6/25\n",
            "20/20 [==============================] - 164s 8s/step - loss: 0.7997 - accuracy: 0.6758 - val_loss: 0.6479 - val_accuracy: 0.5995\n",
            "Epoch 7/25\n",
            "20/20 [==============================] - 163s 8s/step - loss: 0.5708 - accuracy: 0.7528 - val_loss: 0.2185 - val_accuracy: 1.0000\n",
            "Epoch 8/25\n",
            "20/20 [==============================] - 164s 8s/step - loss: 0.5078 - accuracy: 0.7766 - val_loss: 0.2941 - val_accuracy: 0.9839\n",
            "Epoch 9/25\n",
            "20/20 [==============================] - 163s 8s/step - loss: 0.4180 - accuracy: 0.8282 - val_loss: 0.1366 - val_accuracy: 1.0000\n",
            "Epoch 10/25\n",
            "20/20 [==============================] - 163s 8s/step - loss: 0.3503 - accuracy: 0.8647 - val_loss: 0.1207 - val_accuracy: 0.9785\n",
            "Epoch 11/25\n",
            "20/20 [==============================] - 163s 8s/step - loss: 0.4092 - accuracy: 0.8417 - val_loss: 0.1143 - val_accuracy: 1.0000\n",
            "Epoch 12/25\n",
            "20/20 [==============================] - 167s 8s/step - loss: 0.2902 - accuracy: 0.8909 - val_loss: 0.1175 - val_accuracy: 0.9892\n",
            "Epoch 13/25\n",
            "20/20 [==============================] - 164s 8s/step - loss: 0.2998 - accuracy: 0.8885 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
            "Epoch 14/25\n",
            "20/20 [==============================] - 164s 8s/step - loss: 0.1623 - accuracy: 0.9389 - val_loss: 0.0696 - val_accuracy: 0.9758\n",
            "Epoch 15/25\n",
            "20/20 [==============================] - 163s 8s/step - loss: 0.1947 - accuracy: 0.9294 - val_loss: 0.2605 - val_accuracy: 0.8360\n",
            "Epoch 16/25\n",
            "20/20 [==============================] - 167s 8s/step - loss: 0.1729 - accuracy: 0.9321 - val_loss: 0.3862 - val_accuracy: 0.7957\n",
            "Epoch 17/25\n",
            "20/20 [==============================] - 163s 8s/step - loss: 0.1598 - accuracy: 0.9409 - val_loss: 0.0766 - val_accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "12/20 [=================>............] - ETA: 1:00 - loss: 0.1009 - accuracy: 0.9709"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_xFcLxxAOnb"
      },
      "source": [
        "Hacemos un grafico con la precisión de entrenamiento y validación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nHmxIXK3M-W"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j0GZ6ab_Bfx"
      },
      "source": [
        "Este código os va a servir para poder introducir datos en el clasificador:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d6HJoPP3O90"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(fn)\n",
        "  print(classes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}