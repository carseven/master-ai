{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1O5KRefxx04"
      },
      "source": [
        "!pip install datasets\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLQ2NoJy4FFJ"
      },
      "source": [
        "## Entrenamos NN con capa de Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgdgboWdx6cz"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "my_dataset = load_dataset(\"amazon_reviews_multi\", \"es\", split='train')\n",
        "my_dataset = my_dataset.filter(lambda example: example['stars'] in [1, 5])\n",
        "my_dataset = [[example['review_body'], 1 if example['stars'] == 5 else 0] for example in my_dataset]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYJHNTvjyFUo"
      },
      "source": [
        "from tensorflow.keras.preprocessing import text\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "texts = [example[0] for example in my_dataset]\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(texts)\n",
        "X = pad_sequences(X, maxlen=20, padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "039HCZk1yJGd"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "labels = [example[1] for example in my_dataset]\n",
        "y = np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g51-24j9yO_J"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_counts) + 1, 10, input_length=20))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfqm5wEfySoz"
      },
      "source": [
        "model.fit(X, y, batch_size=16, epochs=2, validation_split=0.25, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP2xsTY-4MLU"
      },
      "source": [
        "## Extraemos capa de embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRcra8muypvW"
      },
      "source": [
        "embeddings = model.layers[0].get_weights()[0]\n",
        "word_embeddings = {}\n",
        "\n",
        "for index, vector in enumerate(embeddings):\n",
        "  if not index: # 0 is padding\n",
        "    continue\n",
        "  word = tokenizer.index_word[index]\n",
        "  word_embeddings[word] = vector\n",
        "\n",
        "print(word_embeddings['roto'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9wgC_QtywPW"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(cosine_similarity([word_embeddings['precioso']], [word_embeddings['bonito']]))\n",
        "print(cosine_similarity([word_embeddings['tarde']], [word_embeddings['defectuoso']]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s80CL-IyxWL"
      },
      "source": [
        "print(cosine_similarity([word_embeddings['precioso']], [word_embeddings['defectuoso']]))\n",
        "print(cosine_similarity([word_embeddings['tarde']], [word_embeddings['bonito']]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8s4YdhA6c7_"
      },
      "source": [
        "## Guardamos embeddings en ficheros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOytzTtc6b2_"
      },
      "source": [
        "embeddings_fh = open('my_embeddings.tsv', 'w+')\n",
        "vocabulary_fh = open('my_vocabulary.tsv', 'w+')\n",
        "\n",
        "for word, vector in word_embeddings.items():\n",
        "  embeddings_fh.write('\\t'.join([str(v) for v in vector]) + '\\n')\n",
        "  vocabulary_fh.write(word + '\\n')\n",
        "\n",
        "embeddings_fh.close()\n",
        "vocabulary_fh.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}