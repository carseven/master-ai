{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lematizacion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYM0T6mxF95-"
      },
      "source": [
        "# Importamos modelo para el español"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmbQUM6bFw9u"
      },
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1SaiAWVGQjE"
      },
      "source": [
        "# Tokenizamos y lematizamos texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya0SFf23aJ6w"
      },
      "source": [
        "import spacy\n",
        "\n",
        "my_texts = ['Esta es mi primera oración', 'Vamos a partir estas cadenas de texto en tokens', 'Y vamos a lematizar esos tokens']\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "for text in my_texts:\n",
        "  doc = nlp(text)\n",
        "  for token in doc:\n",
        "    if not token.is_stop:\n",
        "      print(token.lemma_, token.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R_deBfhIt4R"
      },
      "source": [
        "# Importamos dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co74XxMvJA-k"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwZMKpGIJkjy"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"amazon_reviews_multi\", \"es\", split=\"test\")\n",
        "dataset = dataset.filter(lambda example: example['stars'] in [1, 5])\n",
        "dataset = dataset.shuffle()\n",
        "texts = dataset['review_body']\n",
        "stars = dataset['stars']\n",
        "print(len(stars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu3iBUGGStfl"
      },
      "source": [
        "# Tokenizamos y lematizamos texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjJe0JXRSsSk"
      },
      "source": [
        "tokenized_texts = []\n",
        "counter = 0\n",
        "for text in texts:\n",
        "  counter += 1\n",
        "  print('\\rtokenized texts: {}'.format(str(counter)), end='')\n",
        "  tokenized_text = nlp(text)\n",
        "  tokenized_text = [token.lemma_ for token in tokenized_text]\n",
        "  tokenized_texts.append(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfursT-qYc2p"
      },
      "source": [
        "print(texts[0])\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28DiPz5VQhEI"
      },
      "source": [
        "# Creamos Bag of Words (BoW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8_CnwDsJ6KT"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import text\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(tokenized_texts)\n",
        "X = tokenizer.texts_to_matrix(tokenized_texts, mode=\"tfidf\")\n",
        "Y = np.array(list(map(lambda x: 1 if x == 5 else 0, stars)))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Yv5OthLj1E"
      },
      "source": [
        "print(X[0])\n",
        "print(Y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNVb22AsZLzA"
      },
      "source": [
        "# Entrenamos una NN con BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftbipFn9ZwW7"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(250, input_shape=(10000,)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmbTmx9BaV7J"
      },
      "source": [
        "model.fit(X, Y, batch_size=16, epochs=10, validation_split=0.25, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIgJQKI_a90-"
      },
      "source": [
        "texts = ['Este producto es muy bueno', 'Este producto es muy malo']\n",
        "tokenized_texts = [[token.lemma_ for token in nlp(text)] for text in texts]\n",
        "X = tokenizer.texts_to_matrix(tokenized_texts, mode=\"tfidf\")\n",
        "\n",
        "print(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}