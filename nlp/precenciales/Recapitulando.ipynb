{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recapitulando.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9w9N8WHP51G"
      },
      "source": [
        "!pip install datasets\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buAl5pHFPPCh"
      },
      "source": [
        "## Descargamos un dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHBBFAPWMcUW"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "my_dataset = load_dataset(\"amazon_reviews_multi\", \"es\", split='train')\n",
        "my_dataset = my_dataset.filter(lambda example: example['stars'] in [1, 5])\n",
        "my_dataset = [[example['review_body'], 1 if example['stars'] == 5 else 0] for example in my_dataset]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywtfaMxFNQnj"
      },
      "source": [
        "print(len(my_dataset))\n",
        "print(my_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRL6DosxQyVp"
      },
      "source": [
        "import random\n",
        "random.shuffle(my_dataset)\n",
        "my_dataset_1k = my_dataset[:1000]\n",
        "print(len(my_dataset_1k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haTVjJZiPtJV"
      },
      "source": [
        "## Tokenizamos el texto y lo lematizamos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrWUM0qEQpxb"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "texts = [example[0] for example in my_dataset_1k]\n",
        "\n",
        "tokenized_texts = []\n",
        "\n",
        "counter = 0\n",
        "for text in texts:\n",
        "  counter += 1\n",
        "  print('\\rtokenized texts: {}'.format(str(counter)), end='')\n",
        "  doc = nlp(text)\n",
        "  tokenized_text = []\n",
        "  for token in doc:\n",
        "    if token.is_stop or token.is_punct:\n",
        "      continue\n",
        "    tokenized_text.append(token.lemma_)\n",
        "  tokenized_texts.append(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5OLMQSYTP3T"
      },
      "source": [
        "print(len(tokenized_texts))\n",
        "print(texts[0])\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFWLm1pMU6QC"
      },
      "source": [
        "## Representamos el texto como un Bag of Words (BoW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0dJNL0XU5Nz"
      },
      "source": [
        "from tensorflow.keras.preprocessing import text\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(tokenized_texts)\n",
        "X = tokenizer.texts_to_matrix(tokenized_texts, mode=\"tfidf\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMz3fNOIVX0z"
      },
      "source": [
        "print(X.shape)\n",
        "print(X.tolist()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfQ9fVKaWyjD"
      },
      "source": [
        "## Entrenamos una NN con BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpAUDy2HWx1Q"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "labels = [example[1] for example in my_dataset_1k]\n",
        "y = np.array(labels)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlDcs4LlXJUa"
      },
      "source": [
        "print(y.shape)\n",
        "print(y.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu0LYYOmXjmp"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(250, input_shape=(10000,)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA1mcziUXuX8"
      },
      "source": [
        "model.fit(X, y, batch_size=16, epochs=10, validation_split=0.25, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79IL6m7KX7aw"
      },
      "source": [
        "## Predecimos con el modelo entrenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxQgMxzoX_IP"
      },
      "source": [
        "texts = ['Este producto no es muy bueno', 'Este producto es muy malo']\n",
        "tokenized_texts = [[token.lemma_ for token in nlp(text)] for text in texts]\n",
        "X = tokenizer.texts_to_matrix(tokenized_texts, mode=\"tfidf\")\n",
        "\n",
        "print(X.shape)\n",
        "print(X.tolist()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oekq5JxYxWR"
      },
      "source": [
        "scores = model.predict(X)\n",
        "predictions = [1 if score > 0.5 else 0 for score in scores]\n",
        "print(scores)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0bhW9nY8LT"
      },
      "source": [
        "## Representamos el texto como una secuencia de Ã­ndices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwKpJCwyZCSP"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(tokenized_texts)\n",
        "print(X[0])\n",
        "print(len(X[0]))\n",
        "print(len(X[100]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRb_7NxSaoiT"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X = pad_sequences(X, maxlen=50, padding='post', truncating='post')\n",
        "print(X[0])\n",
        "print(X.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy82KEp6lMCk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgynOj1lmRUO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}