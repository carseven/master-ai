# %%
from numpy.lib.function_base import average
import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import os
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from sklearn import svm
import sklearn.metrics as metrics

# Que pandas muestre todas las columnas
pd.set_option("display.max_rows", 30, "display.max_columns", None)
pylab.rcParams['figure.figsize'] = (16.0, 7.0)

# Cargar el dataset
THIS_FOLDER = os.path.dirname(os.path.abspath(__file__))

data = pd.read_csv(THIS_FOLDER + '/datasets_train.csv')
# %% Mostrar las 5 primeras filas del dataframe
data.head(10)

# %% Informa
data.info()

# battery_power: Total energy a battery can store in one time measured in mAh
# blue: Has bluetooth or not
# clock_speed: speed at which microprocessor executes instructions
# dual_sim: Has dual sim support or not
# fc: front camera mega pixeles
# four_g: has 4g or not
# int_memory: Internal Memory in Gigabytes
# m_dep: Mobile Depth in cm
# mobile_wt: Weight of mobile phone
# n_cores: Number of cores of processor
# pc: Primary Camera mega pixels
# px_height: Pixel Resolution Height
# px_width: Pixel Resolution Width
# ram: Random Access Memory in Megabytes
# sc_h: Screen Height of mobile in cm
# sc_w: Screen Width of mobile in cm
# talk_time: longest time that a single battery charge will last when you are
# three_g: Has 3G or not
# touch_screen: Has touch screen or not
# wifi: Has wifi or not
# Columnas a predecir en la clasificacion.
# price_range: This is the target variable with value of
# 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost).
# %% Distinguir entre variables númericas y categoricas

cat_columns = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen',
               'wifi', 'price_range']

num_columns = list(set(data.columns) - set(cat_columns))

print('Variables númericas:')
print(num_columns)
print('\n')
print('Variables categoricas:')
print(cat_columns)
# %% Datos estadísticos de las variables númericas
print('Datos estadíticos de las variables númericas:')
for column in num_columns:
    cuartiles = data[column].quantile([.25, .50, .75])

    print('Variable ' + column + ':')
    print('MAX:', data[column].max())
    print('MIN:', data[column].min())
    print('MEDIA:', data[column].mean())
    print('MEDIANA:', data[column].median())
    print('25%:', cuartiles.iloc[0])
    print('50%:', cuartiles.iloc[1])
    print('75%:', cuartiles.iloc[2])
    print('\n')
# %% Frecuencia de las variables categoricas
for column in cat_columns:
    print(f'Variable {column}:')
    print(f'Listado categorias: \n{data[column].unique()}')
    print(f'Frecuencia categorias: \n{data[column].value_counts()}')
    print('\n')

# %% Distribución de las variables númericas
fig, axs = plt.subplots(len(num_columns), 1, figsize=(15, 50))

count = 0
for column in num_columns:
    fp = sb.distplot(data[column], ax=axs[count])
    fp.set_title(f'Distribución de {column}')
    count += 1

plt.tight_layout()
# %% Distribución de las variables categoricas
fig, axs = plt.subplots(len(cat_columns), 1, figsize=(15, 50))

count = 0
for column in cat_columns:
    fp = sb.countplot(data[column], ax=axs[count])
    fp.set_title(f'Distribución de {column}')
    fp.set_xticklabels(data[column].unique())
    count += 1

# %% Matriz de correlación

# Calcular correlacion de las variables numericas.
df_matrix_corr = data[num_columns].corr(method='pearson').abs()

# Triangulo inferior de la matrix de correlacion
mask = np.triu(np.ones(df_matrix_corr.shape)).astype(np.bool)

# Grafico de la matriz de correlacion
f, ax = plt.subplots(figsize=(15, 20))
ax.set_yticklabels(df_matrix_corr.columns[1:], rotation=0)
ax.set_xticklabels(df_matrix_corr.columns[:-1], rotation=45)
heatmap = sb.heatmap(
    df_matrix_corr,
    mask=mask,
    square=True,
    linewidths=1,
    cmap=sb.light_palette('seagreen', as_cmap=True),
    cbar_kws={'shrink': .4, 'ticks': [-1, -.5, 0, 0.5, 1]},
    vmin=0,
    vmax=1,
    annot=True,
    annot_kws={"size": 10},
    fmt=".2f"
)

# Se aprecia claramente una relación en las variables como pc, fc. sc_w y sc_h,
# px_width y px_px_height
# Mostrar distribuciones
# %% Encontrar las correlacion con que superen el 0.75
umbral = 0.75
corr_stack = df_matrix_corr.where(mask).stack().reset_index()
corr_stack.columns = ['Row', 'Column', 'Correlation Value']
corr_75 = corr_stack[(corr_stack['Correlation Value'] > umbral) &
                     (corr_stack['Correlation Value'] < 1)]
corr_75 = corr_75.sort_values(
    by='Correlation Value',
    kind="quicksort",
    ascending=False).drop_duplicates(keep='first')
print(corr_75)
# %% Porcentage de nulos por columna
percent_null = (data.isnull().sum() / len(data)) * 100
print(percent_null)

# %% Preparar dataset para la clasificación
data['price_range'].unique()  # Tenemos cuatro clases

x = data.drop(columns='price_range')
y = data['price_range']

# Hold out, probar cross validation¿?
x_train, x_test, y_train, y_test = train_test_split(x,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=42)

print("Datos de entrenamiento:")
print(f"Samples: {x_train.shape}")
print(f"Labels: {y_train.shape}\n")
print("Datos de test:")
print(f"Samples: {x_test.shape}")
print(f"Labels: {y_test.shape}\n")

# %% SVM
param_grid = {'C': 10. ** np.arange(-3, 3), 'gamma': 10. ** np.arange(-5, 0)}

svm_model = svm.SVC(kernel='linear', C=1, random_state=42)
svm_model = svm_model.fit(x_train, y_train)
predicted = svm_model.predict(x_test)

# %% Calcular matriz de confusión: TP, TN, FP, FN


def counts_from_confusion(confusion_matrix):
    count = []

    # Iterate through classes and store the counts
    for i in range(confusion_matrix.shape[0]):
        tp = confusion_matrix[i, i]

        fn_mask = np.zeros(confusion_matrix.shape)
        fn_mask[i, :] = 1
        fn_mask[i, i] = 0
        fn = np.sum(np.multiply(confusion_matrix, fn_mask))

        fp_mask = np.zeros(confusion_matrix.shape)
        fp_mask[:, i] = 1
        fp_mask[i, i] = 0
        fp = np.sum(np.multiply(confusion_matrix, fp_mask))

        tn_mask = 1 - (fn_mask + fp_mask)
        tn_mask[i, i] = 0
        tn = np.sum(np.multiply(confusion_matrix, tn_mask))

        count.append({'Clase': i,
                      'TP': tp,
                      'FN': fn,
                      'FP': fp,
                      'TN': tn})

    tp = (count[0]['TP'] + count[1]['TP'] + count[2]
          ['TP'] + count[3]['TP'])
    tn = (count[0]['TN'] + count[1]['TN'] + count[2]
          ['TN'] + count[3]['TN'])
    fp = (count[0]['FP'] + count[1]['FP'] + count[2]
          ['FP'] + count[3]['FP'])
    fn = (count[0]['FN'] + count[1]['FN'] + count[2]
          ['FN'] + count[3]['FN'])

    return tp, tn, fp, fn, count


# %% Metricas de evalucion.
confusion_matrix = metrics.confusion_matrix(y_test, predicted)
tp, tn, fp, fn, dict_confusion = counts_from_confusion(confusion_matrix)

accuracy = metrics.accuracy_score(y_test, predicted)
recall = metrics.recall_score(y_test, predicted, average='micro')
f1 = metrics.f1_score(y_test, predicted, average='micro')
specificity = tn/(tn + fp)
sensibility = tp/(tp + fn)


print("Métricas evaluación:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1: {f1:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"Sensibility: {sensibility:.2f}")

# Visualizar matriz de confusión
metrics.plot_confusion_matrix(svm_model, x_test, y_test)
plt.show()
print(dict_confusion)
# %% SVM Cross-validation
# gridsearch
svm_model = svm.SVC(kernel='linear', C=1, random_state=42)
scores = cross_val_score(svm_model, x_train, y_train)
predicted = cross_val_predict(svm_model, x_test, y_test)
# %%
confusion_matrix = metrics.confusion_matrix(y_test, predicted)
tp, tn, fp, fn, dict_confusion = counts_from_confusion(confusion_matrix)

accuracy = metrics.accuracy_score(y_test, predicted)
recall = metrics.recall_score(y_test, predicted, average='micro')
f1 = metrics.f1_score(y_test, predicted, average='micro')
specificity = tn/(tn + fp)
sensibility = tp/(tp + fn)


print("Métricas evaluación:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1: {f1:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"Sensibility: {sensibility:.2f}")

fig, ax = plt.subplots()
ax = sb.heatmap(
    confusion_matrix,
    fmt='.4g',
    annot=True,
    annot_kws={'size': 16},
    cmap='Blues_r',
    cbar=False)
plt.title('Matriz de confusión')
plt.ylabel('Clase actual')
plt.xlabel('Clase de predicción')
plt.ylim(4, 0)
plt.show()
print(dict_confusion)
# %% Neural networks
